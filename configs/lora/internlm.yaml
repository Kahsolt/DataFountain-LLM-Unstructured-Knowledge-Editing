alg_name: LoRA
model_name: internlm/internlm2_5-1_8b-chat
device: 0

lora_type: "adalora"
layers: [7, 15, 23]
num_steps: 20
batch_size: 4
max_length: 512
lr: 5e-3
weight_decay: 0
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0.1
norm_constraint: false
target_modules: ["wqkv", "wo"]
#target_modules: ["w1", "w2", "w3"]
model_parallel: false
