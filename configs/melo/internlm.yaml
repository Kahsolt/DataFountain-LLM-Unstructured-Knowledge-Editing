alg_name: MELO
device: 0

model_name: internlm/internlm2_5-1_8b-chat
model_parallel: false
max_length: 512

task: hall
lora_task_type: CAUSAL_LM

check_dir: null

grace:
  name: grace
  num_iter: 50
  init_radius: 0.5
  dist_fn: euc # euc, mmd, cos
  val_init: cold # cold, warm
  val_train: sgd # sgd, pert
  val_reg: None # early
  reg: early_stop # early_stop
  replacement: replace_prompt # replace_last, replace_all, replace_prompt
  expand_mode: moving_avg # , moving_avg, decay
  num_pert: 8 # only matters when using perturbation training
  key_id: -1
  num_edit_per_block: 4
  num_block: 350
  num_rank_per_block: 2
  metric_period: 400
  edit_lr: 0.001
model:
  name: internlm/internlm2_5-1_8b-chat
  class_name: AutoModel
  tokenizer_name: internlm/internlm2_5-1_8b-chat
  tokenizer_class: AutoTokenizer
  fan_in_fan_out: True
  target_modules:
    - model.layers.23.feed_forward.w1
    - model.layers.23.feed_forward.w2
    - model.layers.23.feed_forward.w3
  pt: ./checkpoint  # set this to 'hallucination' inside your checkpoint directory
  grace_layer: model.layers.22.feed_forward.w3
lora:
  cls_name: distilbert/distilbert-base-cased
  cls_class: AutoModel
  supervised: true
  cos: false
  freeze: null
  square: true
  bound_embeds: false
  use_all_negatives: false
  freeze_lora: false
  dist_heads: 1
  cross_attend: false
  soft_weighting: false
  checkpoint_grad: false
  lora_r: 64
  lora_alpha: 64
  lora_dropout: 0.0
